# More about data integrity and compliance

This reading illustrates the importance of data integrity using an example of a global company’s data. Definitions of terms that are relevant to data integrity will be provided at the end.

## Scenario: calendar dates for a global company

Calendar dates are represented in a lot of different short forms. Depending on where you live, a different format might be used.

* In some countries, **12/10/20** (DD/MM/YY) stands for October 12, 2020.
* In other countries, the national standard is YYYY-MM-DD so October 12, 2020 becomes **2020-10-12**.
* In the United States, (MM/DD/YY) is the accepted format so October 12, 2020 is going to be **10/12/20**.

Now, think about what would happen if you were working as a data analyst for a global company and didn’t check date formats. Well, your data integrity would probably be questionable. Any analysis of the data would be inaccurate. Imagine ordering extra inventory for December when it was actually needed in October!

A good analysis depends on the integrity of the data, and data integrity usually depends on using a common format. So it is important to double-check how dates are formatted to make sure what you think is December 10, 2020 isn’t really October 12, 2020, and vice versa.

Here are some other things to watch out for:

* **Data replication compromising data integrity:** Continuing with the example, imagine you ask your international counterparts to verify dates and stick to one format. One analyst copies a large dataset to check the dates. But because of memory issues, only part of the dataset is actually copied. The analyst would be verifying and standardizing incomplete data. That partial dataset would be certified as compliant but the full dataset would still contain dates that weren't verified. Two versions of a dataset can introduce inconsistent results. A final audit of results would be essential to reveal what happened and correct all dates.
* **Data transfer compromising data integrity:** Another analyst checks the dates in a spreadsheet and chooses to import the validated and standardized data back to the database. But suppose the date field from the spreadsheet was incorrectly classified as a text field during the data import (transfer) process. Now some of the dates in the database are stored as text strings. At this point, the data needs to be cleaned to restore its integrity.
* **Data manipulation compromising data integrity:** When checking dates, another analyst notices what appears to be a duplicate record in the database and removes it. But it turns out that the analyst removed a unique record for a company’s subsidiary and not a duplicate record for the company. Your dataset is now missing data and the data must be restored for completeness.

## Conclusion

Fortunately, with a standard date format and compliance by all people and systems that work with the data, data integrity can be maintained. But no matter where your data comes from, always be sure to check that it is valid, complete, and clean before you begin any analysis.

![A trio of icons: A lightbulb with a checkmark, a checklist, and a person on a laptop](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/MxSHaCOGS12Uh2gjhjtdBQ_067803a06f4243f48af071a66805ff3f_Screen-Shot-2021-01-25-at-12.03.17-PM.png?expiry=1750896000000&hmac=XbcKGhm4sLLzOel2TnU3VBuvq3vntAgj9lgTXSP2Mew)

## Reference: Data constraints and examples

As you progress in your data journey, you'll come across many types of data constraints (or criteria that determine validity). The  table below offers definitions and examples of data constraint terms you might come across.

| **Data constraint**                     | Definition                                                                                       | Examples                                                                                                                                                                                                                                                      |
| ----------------------------------------- | -------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Data type**                           | Values must be of a certain type: date, number, percentage, Boolean, etc.                        | If the data type is a date, a single number like 30 would fail the constraint and be invalid                                                                                                                                                                  |
| **Data range**                          | Values must fall between predefined maximum and minimum values                                   | If the data range is 10-20, a value of 30 would fail the constraint and be invalid                                                                                                                                                                            |
| **Mandatory**                           | Values can’t be left blank or empty                                                             | If age is mandatory, that value must be filled in                                                                                                                                                                                                             |
| **Unique**                              | Values can’t have a duplicate                                                                   | Two people can’t have the same mobile phone number within the same service area                                                                                                                                                                              |
| **Regular expression (regex) patterns** | Values must match a prescribed pattern                                                           | A phone number must match ###-###-#### (no other characters allowed)                                                                                                                                                                                          |
| **Cross-field validation**              | Certain conditions for multiple fields must be satisfied                                         | Values are percentages and values from multiple fields must add up to 100%                                                                                                                                                                                    |
| **Primary-key**                         | (Databases only) value must be unique per column                                                 | A database table can’t have two rows with the same primary key value. A primary key is an identifier in a database that references a column in which each value is unique. More information about primary and foreign keys is provided later in the program. |
| **Set-membership**                      | (Databases only) values for a column must come from a set of discrete values                     | Value for a column must be set to Yes, No, or Not Applicable                                                                                                                                                                                                  |
| **Foreign-key**                         | (Databases only) values for a column must be unique values coming from a column in another table | In a U.S. taxpayer database, the State column must be a valid state or territory with the set of acceptable values defined in a separate States table                                                                                                         |
| **Accuracy**                            | The degree to which the data conforms to the actual entity being measured or described           | If values for zip codes are validated by street location, the accuracy of the data goes up.                                                                                                                                                                   |
| **Completeness**                        | The degree to which the data contains all desired components or measures                         | If data for personal profiles required hair and eye color, and both are collected, the data is complete.                                                                                                                                                      |
| **Consistency**                         | The degree to which the data is repeatable from different points of entry or collection          | If a customer has the same address in the sales and repair databases, the data is consistent.                                                                                                                                                                 |

# Well-aligned objectives and data

You can gain powerful insights and make accurate conclusions when data is well-aligned to business objectives. As a data analyst, alignment is something you will need to judge. Good alignment means that the data is relevant and can help you solve a business problem or determine a course of action to achieve a given business objective.

In this reading, you will review the business objectives associated with three scenarios. You will explore how clean data and well-aligned business objectives can help you come up with accurate conclusions. On top of that, you will learn how new variables discovered during data analysis can cause you to set up data constraints so you can keep the data aligned to a business objective.

## Clean data + alignment to business objective = accurate conclusions

### **Business objective**

Account managers at Impress Me, an online content subscription service, want to know how soon users view content after their subscriptions are activated.

![Image of a web video player and image of an analog clock set to 3:03 ](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/uGU6wRn8SXClOsEZ_NlwUQ_1f46ab637fc641acafae6f862c23fc46_Screen-Shot-2021-01-25-at-12.38.21-PM.png?expiry=1750896000000&hmac=ldc4Ier9KTuTo1GnzpNKtp64lCeseikOMOTD-XLseC4)

To start off, the data analyst verifies that the data exported to spreadsheets is clean and confirms that the data needed (when users access content) is available. Knowing this, the analyst decides there is good alignment of the data to the business objective. All that is missing is figuring out exactly how long it takes each user to view content after their subscription has been activated.

Here are the data processing steps the analyst takes for a user from an account called V&L Consulting. (These steps would be repeated for each subscribing account, and for each user associated with that account.)

### **Step 1**

| Data-processing step                           | Source of data      |
| ------------------------------------------------ | --------------------- |
| Look up the activation date for V&L Consulting | Account spreadsheet |

**Relevant data in spreadsheet:**

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/36M6eOT6ScmLM-ThsQ2yXw_9d477d22cbc64c8695d010e87c1336f1_DA_C4M1_well-aligned_objectives_1.png?expiry=1750896000000&hmac=NSYnycG8EXMl5HDZE6WU-r9cUmK3N5rXmlkIOx9kRWA)

**Result**: October 21, 2019

### **Step 2**

| Data-processing step                                               | Source of data                  |
| -------------------------------------------------------------------- | --------------------------------- |
| Look up the name of a user belonging to the V&L Consulting account | Account spreadsheet (users tab) |

**Relevant data in spreadsheet**:

![Screenshot of two columns of a spreadsheet for Account Name and Users. The name Maria Ballantyne is highlighted](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/R-nkA9w1Riep5APcNSYnRA_7480900c4f204ffb9421199fd8bf32c8_Screen-Shot-2021-01-18-at-6.27.24-PM.png?expiry=1750896000000&hmac=bMmuevxC3WDei7QoAHu5mJ7kPK0RpF0fSQp16OriaqE)

**Result**: Maria Ballantyne

### **Step 3**

| Data-processing step                            | Source of data            |
| ------------------------------------------------- | --------------------------- |
| Find the first content access date for Maria B. | Content usage spreadsheet |

**Relevant data in spreadsheet:**

![Screenshot of two columns of a spreadsheet for Users and Access Dates. The date 10/31/2019 is highlighted](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/_Mn53wNpR0aJ-d8DaedG4w_ecc7521519ea4b4c810dd80bc0f54b9d_Screen-Shot-2021-01-18-at-6.35.48-PM.png?expiry=1750896000000&hmac=pXM1jvsEDHd1phLbAFRF9GFO4_LS798HreSfgwlC8RQ)

**Result**: October 31, 2019

### **Step 4**

| Data-processing step                                                       | Source of data              |
| ---------------------------------------------------------------------------- | ----------------------------- |
| Calculate the time between activation and first content usage for Maria B. | New spreadsheet calculation |

**Relevant data in spreadsheet**:

![Screenshot of spreadsheet with Account, Users, Activation Date, 1st Access Date, and Number of Days. Number 10 is highlighted](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/jV8hcatCTq2fIXGrQr6ttw_b4f3452496b543bd9e46716485d5175e_Screen-Shot-2021-01-18-at-6.41.56-PM.png?expiry=1750896000000&hmac=K5SglvkeAArG0iREudyrtdIvqzia6SxFpcRdm3w6yUQ)

**Result**: 10 days

### **Pro tip 1**

In the above process, the analyst could use VLOOKUP to look up the data in Steps 1, 2, and 3 to populate the values in the spreadsheet in Step 4. [VLOOKUP](https://support.microsoft.com/en-us/office/vlookup-function-0bbc8083-26fe-4963-8ab8-93a18ad188a1 "This link takes you to the Microsoft Support page for the VLOOKUP function.")is a spreadsheet function that searches for a certain value in a column to return a related piece of information. Using VLOOKUP can save a lot of time; without it, you have to look up dates and names manually.

Refer to the [VLOOKUP](https://support.google.com/docs/answer/3093318?hl=en "This link takes you to the VLOOKUP help page for Google Sheets.")page in the Google Help Center for how to use the function in Google Sheets.

### **Pro tip 2**

In Step 4 of the above process, the analyst could use the **DATEDIF** function to automatically calculate the difference between the dates in column C and column D. The function can calculate the number of days between two dates.

Refer to the Microsoft Support [DATEDIF](https://support.microsoft.com/en-us/office/datedif-function-25dba1a4-2812-480b-84dd-8b32a451b35c "DATEDIF") page for how to use the function in Excel. The [DAYS360](https://support.microsoft.com/en-us/office/days360-function-b9a509fd-49ef-407e-94df-0cbda5718c2a "DAYS360")function does the same thing in accounting spreadsheets that use a 360-day year (twelve 30-day months).

Refer to the [DATEDIF](https://support.google.com/docs/answer/6055612?hl=en "This link takes you to the Google Help Center page for the DATEIF function.") page in the Google Help Center for how to use the function in Google Sheets.

## Alignment to business objective + additional data cleaning = accurate conclusions

### **Business objective**

Cloud Gate, a software company, recently hosted a series of public webinars as free product introductions. The data analyst and webinar program manager want to identify companies that had five or more people attend these sessions. They want to give this list of companies to sales managers who can follow up for potential sales.

![An image of a group of people chatting and an image of an online page](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/kfJtDm53S4iybQ5ud9uIaQ_c2812e7377fe4f48bc044561996b3d92_Screen-Shot-2021-01-25-at-12.46.54-PM.png?expiry=1750896000000&hmac=lslfwIlHb6OZor7b42da7TKNXj8PtASTxpkEmcw3Xag)

The webinar attendance data includes the fields and data shown below.

| **Name**          | <*First name*> <*Last name*> | This was required information attendees had to submit |
| ------------------- | ------------------------------ | ------------------------------------------------------- |
| **Email Address** | xxxxx@*company*.com          | This was required information attendees had to submit |
| **Company**       | <*Company name*>             | This was optional information attendees could provide |
|                   |                              |                                                       |

### **Data cleaning**

The webinar attendance data seems to align with the business objective. But the data analyst and program manager decide that some data cleaning is needed before the analysis. They think data cleaning is required because:

* The company name wasn’t a mandatory field. If the company name is blank, it might be found from the email address. For example, if the email address is `username@google.com`, the company field could be filled in with Google for the data analysis. This data cleaning step assumes that people with company-assigned email addresses attended a webinar for business purposes.
* Attendees could enter any name. Since attendance across a series of webinars is being looked at, they need to validate names against unique email addresses. For example, if Joe Cox attended two webinars but signed in as Joe Cox for one and Joseph Cox for the other, he would be counted as two different people. To prevent this, they need to check his unique email address to determine that he was the same person. After the validation, Joseph Cox could be changed to Joe Cox to match the other instance.

## Alignment to business objective + newly discovered variables + constraints = accurate conclusions

### **Business objective**

An after-school tutoring company, A+ Education,  wants to know if there is a minimum number of tutoring hours needed before students have at least a 10% improvement in their assessment scores.

![An image of a person writing on a pad and an image of a gauge meter](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/XIgx4gCvR5-IMeIAr2efDA_a28452c4b7874cd68547049e163f0c2b_Screen-Shot-2021-01-25-at-12.52.43-PM.png?expiry=1750896000000&hmac=YG6xek9s_DYJb0rT50qYFf9XvXPp4ugBxNaUU_ZXN0Y)

The data analyst thinks there is good alignment between the data available and the business objective because:

* Students log in and out of a system for each tutoring session, and the number of hours is tracked
* Assessment scores are regularly recorded

### **Data constraints for new variables**

After looking at the data, the data analyst discovers that there are other variables to consider. Some students had consistent weekly sessions while other students had scheduled sessions more randomly even though their total number of tutoring hours was the same. The data doesn’t align as well with the original business objective as first thought, so the analyst adds a data constraint to focus only on the students with consistent weekly sessions. This modification helps to get a more accurate picture about the enrollment time needed to achieve a 10% improvement in assessment scores.

# When you find an issue with your data

When you are getting ready for data analysis, you might realize you don’t have the data you need or you don’t have enough of it. In some cases, you can use what is known as proxy data in place of the real data. Think of it like substituting oil for butter in a recipe when you don’t have butter. In other cases, there is no reasonable substitute and your only option is to collect more data.

Consider the following data issues and suggestions on how to work around them.

## Data issue 1: no data

| Possible Solutions                                                                                                                                               | Examples of solutions in real life                                                                                                                                                                          |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Gather the data on a small scale to perform a preliminary analysis and then request additional time to complete the analysis after you have collected more data. | If you are surveying employees about what they think about a new performance and bonus plan, use a sample for a preliminary analysis. Then, ask for another 3 weeks to collect the data from all employees. |
| If there isn’t time to collect data, perform the analysis using proxy data from other datasets.                                                                 |                                                                                                                                                                                                             |
| *This is the most common workaround.*                                                                                                                            | If you are analyzing peak travel times for commuters but don’t have the data for a particular city, use the data from another city with a similar size and demographic.                                    |

## Data issue 2: too little data

| Possible Solutions                                            | Examples of solutions in real life                                                                                                                                               |
| --------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Do the analysis using proxy data along with actual data.      | If you are analyzing trends for owners of golden retrievers, make your dataset larger by including the data from owners of labradors.                                            |
| Adjust your analysis to align with the data you already have. | If you are missing data for 18- to 24-year-olds, do the analysis but note the following limitation in your report:*this conclusion applies to adults 25 years and older* *only*. |

## Data issue 3: wrong data, including data with errors*

| Possible Solutions                                                                                                                                                                                     | Examples of solutions in real life                                                                                                                                                                |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| If you have the wrong data because requirements were misunderstood, communicate the requirements again.                                                                                                | If you need the data for female voters and received the data for male voters, restate your needs.                                                                                                 |
| Identify errors in the data and, if possible, correct them at the source by looking for a pattern in the errors.                                                                                       | If your data is in a spreadsheet and there is a conditional statement or boolean causing calculations to be wrong, change the conditional statement instead of just fixing the calculated values. |
| If you can’t correct data errors yourself, you can ignore the wrong data and go ahead with the analysis if your sample size is still large enough and ignoring the data won’t cause systematic bias. | If your dataset was translated from a different language and some of the translations don’t make sense, ignore the data with bad translation and go ahead with the analysis of the other data.   |

***Important note:*** Sometimes data with errors can be a warning sign that the data isn’t reliable. Use your best judgment.

### Use the following decision tree as a reminder of how to deal with data errors or not enough data:

![This illustration is a decision tree showing four possible decisions to make in order to work around data issues.](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/nubavN6IS5mm2rzeiFuZgw_1204106238b34cff9a89859772cdfaa1_Screen-Shot-2021-03-05-at-10.36.19-AM.png?expiry=1750896000000&hmac=Bl2v382BWj2vNNGVnw_mvcv-y2JCw22RaXAPl6Ab2So)

# Calculate sample size

Before you dig deeper into sample size, familiarize yourself with these terms and definitions:

| **Terminology**              | **Definitions**                                                                                                                                                                                                                                                                                                                                                                       |
| ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Population**               | The entire group that you are interested in for your study. For example, if you are surveying people in your company, the population would be all the employees in your company.                                                                                                                                                                                                      |
| **Sample**                   | A subset of your population. Just like a food sample, it is called a sample because it is only a taste. So if your company is too large to survey every individual, you can survey a representative sample of your population.                                                                                                                                                        |
| **Margin of error**          | Since a sample is used to represent a population, the sample’s results are expected to differ from what the result would have been if you had surveyed the entire population. This difference is called the margin of error. The smaller the margin of error, the closer the results of the sample are to what the result would have been if you had surveyed the entire population. |
| **Confidence level**         | How confident you are in the survey results. For example, a 95% confidence level means that if you were to run the same survey 100 times, you would get similar results 95 of those 100 times. Confidence level is targeted before you start your study because it will affect how big your margin of error is at the end of your study.                                              |
| **Confidence interval**      | The range of possible values that the population’s result would be at the confidence level of the study. This range is the sample result +/- the margin of error.                                                                                                                                                                                                                    |
| **Statistical significance** | The determination of whether your result could be due to random chance or not. The greater the significance, the less due to chance.                                                                                                                                                                                                                                                  |

## Things to remember when determining the size of your sample

When figuring out a sample size, here are things to keep in mind:

* **Don’t use a sample size less than 30**. It has been statistically proven that 30 is the smallest sample size where an average result of a sample starts to represent the average result of a population.
* **The confidence level most commonly used is 95%**, but 90% can work in some cases.

Increase the sample size to meet specific needs of your project:

* For a **higher** confidence level, use a larger sample size
* To **decrease** the margin of error, use a larger sample size
* For **greater** statistical significance, use a larger sample size

**Note:** Sample size calculators use statistical formulas to determine a sample size. More about these are coming up in the course!  Stay tuned.

### **Why a minimum sample of 30?**

This recommendation is based on the **Central Limit Theorem (CLT)** in the field of probability and statistics. As sample size increases, the results more closely resemble the normal (bell-shaped) distribution from a large number of samples. A sample of 30 is the smallest sample size for which the CLT is still valid. Researchers who rely on **regression analysis** – **statistical methods to determine the relationships between controlled and dependent variables –** also prefer a minimum sample of 30.

Still curious? Without getting too much into the math, check out these articles:

* [Central Limit Theorem (CLT)](https://www.investopedia.com/terms/c/central_limit_theorem.asp "Central Limit Theorem (CLT)"): This article by Investopedia explains the Central Limit Theorem and briefly describes how it can apply to an analysis of a stock index.
* [Sample Size Formula](https://www.statisticssolutions.com/dissertation-resources/sample-size-calculation-and-sample-size-justification/sample-size-formula/ "Sample Size Formula"): This article by Statistics Solutions provides a little more detail about why some researchers use 30 as a minimum sample size.

## Sample sizes vary by business problem

Sample size will vary based on the type of business problem you are trying to solve.

For example, if you live in a city with a population of 200,000 and get 180,000 people to respond to a survey, that is a large sample size. But without actually doing that, what would an acceptable, smaller sample size look like?

Would 200 be alright if the people surveyed represented every district in the city?

**Answer**: It depends on the stakes.

* A sample size of 200 might be large enough if your business problem is to find out how residents felt about the new library
* A sample size of 200 might not be large enough if your business problem is to determine how residents would vote to fund the library

You could probably accept a larger margin of error surveying how residents feel about the new library versus surveying residents about how they would vote to fund it. For that reason, you would most likely use a larger sample size for the voter survey.

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/gV3IDnBGQpaYC8NQq99LUA_882037bad438422188c164eec37cf4f1_koe1D0gdTe-HtQ9IHc3v5w_53d654b1c8a54cc6a06fefceea859509_Screen-Shot-2021-01-25-at-1.10.03-PM.png?expiry=1750896000000&hmac=_uB7PDWoAJuZpFSdqD_-0sJ3rPhL0pJzfuS7U6GQtoM)

## Larger sample sizes have a higher cost

You also have to weigh the cost against the benefits of more accurate results with a larger sample size. Someone who is trying to understand consumer preferences for a new line of products wouldn’t need as large a sample size as someone who is trying to understand the effects of a new drug. For drug safety, the benefits outweigh the cost of using a larger sample size. But for consumer preferences, a smaller sample size at a lower cost could provide good enough results.

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/cns9CGjPQE-VGIRhdAfxRA_0d5bc92ec4814185ae6799236607bdf1_37e3kQnbQA63t5EJ21AOUA_abd26b52acfb49f796bb8ba21783cea1_Screen-Shot-2021-01-25-at-1.08.09-PM.png?expiry=1750896000000&hmac=RG_YxoVz7Iutb5ZCqSOENsNM50bowLfkoeb8eDdI-kY)

## Knowing the basics is helpful

Knowing the basics will help you make the right choices when it comes to sample size. You can always raise concerns if you come across a sample size that is too small. A sample size calculator is also a great tool for this. Sample size calculators let you enter a desired confidence level and margin of error for a given population size. They then calculate the sample size needed to statistically achieve those results.

Refer to the [Determine the Best Sample Size](https://www.coursera.org/learn/process-data/lecture/mSj5A/determine-the-best-sample-size "This link takes you to the Determine the Best Sample Size video in this course.") video for a demonstration of a sample size calculator, or refer to the [Sample Size Calculator](https://www.coursera.org/learn/process-data/supplement/ZqcDw/sample-size-calculator "This link takes you to the Sample Size Calculator reading in this course.") reading for additional information.

![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/_1csR-oLQ-mKyS5GpVkilA_6760f1fef2f343388005fbca6fecc9f1_R4RY1zbET-OEWNc2xI_jrA_dacca9cef1ea4417ad9ffdff1e7ca869_Screen-Shot-2021-01-25-at-1.13.19-PM.png?expiry=1750896000000&hmac=LAhh1PmluuZveEYMtbILY2wH6ji_oORoRg9hOnFr8gU)

# When data isn't readily available

Earlier, you learned how you can still do an analysis using proxy data if you have no data. You might have some questions about proxy data, so this reading will give you a few more examples of the types of datasets that can serve as alternate data sources.

## Proxy data examples

Sometimes the data to support a business objective isn’t readily available. This is when proxy data is useful. Take a look at the following scenarios and where proxy data comes in for each example:

| **Business scenario**                                                                                                                                                       | **How proxy data can be used**                                                                                                                       |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| A new car model was just launched a few days ago and the auto dealership can’t wait until the end of the month for sales data to come in. They want sales projections now. | The analyst proxies the number of clicks to the car specifications on the dealership’s website as an estimate of potential sales at the dealership. |
| A brand new plant-based meat product was only recently stocked in grocery stores and the supplier needs to estimate the demand over the next four years.                    | The analyst proxies the sales data for a turkey substitute made out of tofu that has been on the market for several years.                           |
| The Chamber of Commerce wants to know how a tourism campaign is going to impact travel to their city, but the results from the campaign aren’t publicly available yet.     | The analyst proxies the historical data for airline bookings to the city one to three months after a similar campaign was run six months earlier.    |

## Open (public) datasets

If you are part of a large organization, you might have access to lots of sources of data. But if you are looking for something specific or a little outside your line of business, you can also make use of open or public datasets. (You can refer to this [Medium article](https://medium.com/thinkdata/is-there-a-difference-between-open-data-and-public-data-2b8d5608b2f1 "medium article difference between open data and public data") for a brief explanation of the difference between open and public data.)

Here's an example. A nasal version of a vaccine was recently made available. A clinic wants to know what to expect for contraindications, but just started collecting first-party data from its patients. A **contraindication** is a condition that may cause a patient not to take a vaccine due to the harm it would cause them if taken. To estimate the number of possible contraindications, a data analyst proxies an open dataset from a trial of the injection version of the vaccine. The analyst selects a subset of the data with patient profiles most closely matching the makeup of the patients at the clinic.

There are plenty of ways to share and collaborate on data within a community. Kaggle ([kaggle.com](https://www.kaggle.com/ "This link takes you to the Kaggle home page.")) which we previously introduced, has datasets in a variety of formats including the most basic type, Comma Separated Values (CSV) files.

![An image of a magnifying glass and an image of a puzzle piece being added to a puzzle](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/oJRBo8f_QFaUQaPH__BWAg_969ab40fa7a241f39b00e7c3e2e69cc9_Screen-Shot-2021-01-25-at-1.27.09-PM.png?expiry=1750896000000&hmac=UOMJWde0RhQlxrc4KZorxH2nsEWhQXs1pQrxTksdrlA)

### **CSV, JSON, SQLite, and BigQuery datasets**

* CSV: Check out this [Credit card customers](https://www.kaggle.com/sakshigoyal7/credit-card-customers "This link takes you to a Kaggle dataset with anonymized credit card data. ") dataset, which has information from 10,000 customers including age, salary, marital status, credit card limit, credit card category, etc. (CC0: Public Domain, Sakshi Goyal).
* JSON: Check out this JSON dataset for [trending YouTube videos](https://www.kaggle.com/datasnaek/youtube-new "This link takes you to a Kaggle dataset for trending YouTube videos.") (CC0: Public Domain, Mitchell J).
* SQLite: Check out this SQLite dataset for 24 years worth of [U.S. wildfire data](https://www.kaggle.com/rtatman/188-million-us-wildfires "This link takes you to a Kaggle dataset for U.S. wildfires.") (CC0: Public Domain, Rachael Tatman).
* BigQuery: Check out this [Google Analytics 360](https://www.kaggle.com/bigquery/google-analytics-sample "This link takes you to a sample Google Analytics dataset in Kaggle.") sample dataset from the Google Merchandise Store (CC0 Public Domain, Google BigQuery).

Refer to the Kaggle [documentation for datasets](https://www.kaggle.com/docs/datasets "This link takes you to the Kaggle documentation for datasets.") for more information and search for and explore datasets on your own at [kaggle.com/datasets](https://www.kaggle.com/datasets "This link takes you to the main Kaggle datasets page.").

As with all other kinds of datasets, be on the lookout for duplicate data and ‘Null’ in open datasets. Null most often means that a data field was unassigned (left empty), but sometimes Null can be interpreted as the value, 0. It is important to understand how Null was used before you start analyzing a dataset with Null data.

# Sample size calculator

In this reading, you will learn the basics of sample size calculators, how to use them, and how to understand the results. A **sample size calculator** tells you how many people you need to interview (or things you need to test) to get results that represent the target population. Let’s review some terms you will come across when using a sample size calculator:

* **Confidence level**: The probability that your sample size accurately reflects the greater population.
* **Margin of error**: The maximum amount that the sample results are expected to differ from those of the actual population.
* **Population**: This is the total number you hope to pull your sample from.
* **Sample**: A part of a population that is representative of the population.
* **Estimated response rate**: If you are running a survey of individuals, this is the percentage of people you expect will complete your survey out of those who received the survey.

## How to use a sample size calculator

In order to use a sample size calculator, you need to have the population size, confidence level, and the acceptable margin of error already decided so you can input them into the tool. If this information is ready to go, check out these sample size calculators below:

* [Sample size calculator by surveymonkey.com](https://www.surveymonkey.com/mp/sample-size-calculator/ "This link takes you to a sample size calculator created by SurveyMonkey.")
* [Sample size calculator by raosoft.com](http://www.raosoft.com/samplesize.html "This liink takes you to a sample size calculator created by Raosoft. ")

## What to do with the results

After you have plugged your information into one of these calculators, it will give you a recommended sample size. Keep in mind, the calculated sample size is the **minimum** number to achieve what you input for confidence level and margin of error. If you are working with a survey, you will also need to think about the estimated response rate to figure out how many surveys you will need to send out. For example, if you need a sample size of 100 individuals and your estimated response rate is 10%, you will need to send your survey to 1,000 individuals to get the 100 responses you need for your analysis.

Now that you have the basics, try some calculations using the sample size calculators and refer back to this reading if you need a refresher on the definitions.

# All about margin of error

**Margin of error** is the maximum amount that the sample results are expected to differ from those of the actual population. More technically, the margin of error defines a range of values below and above the average result for the sample. The average result for the entire population is expected to be within that range. We can better understand margin of error by using some examples below.

## Margin of error in baseball

![An image of a baseball batter hitting a baseball with an umpire kneeling behind him](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/eBY3N-ZbSruWNzfmW3q7Eg_106e5f1b1d184d87890158ba1221f9e4_Screen-Shot-2021-01-25-at-1.31.14-PM.png?expiry=1750896000000&hmac=cwa47cNn9TYjbFsPS8__MnQFFuNjff91DrCIqjAODvA)

Imagine you are playing baseball and that you are up at bat. The crowd is roaring, and you are getting ready to try to hit the ball. The pitcher delivers a fastball traveling about 90-95mph, which takes about 400 milliseconds (ms) to reach the catcher’s glove. You swing and miss the first pitch because your timing was a little off. You wonder if you should have swung slightly earlier or slightly later to hit a home run. That time difference can be considered the margin of error, and it tells us how close or far your timing was from the average home run swing.

## Margin of error in marketing

The margin of error is also important in marketing. Let’s use A/B testing as an example. **A/B testing** (or split testing) tests two variations of the same web page to determine which page is more successful in attracting user traffic and generating revenue. User traffic that gets monetized is known as the **conversion rate**. A/B testing allows marketers to test emails, ads, and landing pages to find the data behind what is working and what isn’t working. Marketers use the **confidence interval** (determined by the conversion rate and the margin of error) to understand the results.

For example, suppose you are conducting an A/B test to compare the effectiveness of two different email subject lines to entice people to open the email. You find that subject line A: “Special offer just for you” resulted in a 5% open rate compared to subject line B: “Don’t miss this opportunity” at 3%.

Does that mean subject line A is better than subject line B? It depends on your margin of error. If the margin of error was 2%, then subject line A’s actual open rate or confidence interval is somewhere between 3% and 7%. Since the lower end of the interval overlaps with subject line B’s results at 3%, you can’t conclude that there is a statistically significant difference between subject line A and B. Examining the margin of error is important when making conclusions based on your test results.

## Want to calculate your margin of error?

All you need is population size, confidence level, and sample size. In order to better understand this calculator, review these terms:

* **Confidence level**: A percentage indicating how likely your sample accurately reflects the greater population
* **Population**: The total number you pull your sample from
* **Sample**: A part of a population that is representative of the population
* **Margin of error**: The maximum amount that the sample results are expected to differ from those of the actual population

In most cases, a 90% or 95% confidence level is used. But, depending on your industry, you might want to set a stricter confidence level. A 99% confidence level is reasonable in some industries, such as the pharmaceutical industry.

After you have settled on your population size, sample size, and confidence level, plug the information into a margin of error calculator like the ones below:

* [Margin of error calculator by Good Calculators (free online calculators)](https://goodcalculators.com/margin-of-error-calculator/ "This link takes you to a free margin of error calculator by Good Calculators.")
* [Margin of error calculator by CheckMarket](https://www.checkmarket.com/sample-size-calculator/#sample-size-margin-of-error-calculator "This link takes you to a margin of error calculator by CheckMarket.")

# What is dirty data?

Earlier, we discussed that **dirty data** is data that is incomplete, incorrect, or irrelevant to the problem you are trying to solve.  This reading summarizes:

* Types of dirty data you may encounter
* What may have caused the data to become dirty
* How dirty data is harmful to businesses

## Types of dirty data

![Icons of the 6 types of dirty data: duplicate, outdated, incomplete, incorrect and inconsistent data](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/apsqUaUcTWabKlGlHK1mGw_2d181a03a4c6408aa1b7be625c6f9c97_Screen-Shot-2021-01-24-at-11.51.49-PM.png?expiry=1750896000000&hmac=EwAZDRhWUP8CXFBPNoAEZLuK0lFc5OUDKOL6qhtQIXg)

### **Duplicate data**

| Description                                  | Possible causes                                          | Potential harm to businesses                                                                                 |
| ---------------------------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| Any data record that shows up more than once | Manual data entry, batch data imports, or data migration | Skewed metrics or analyses, inflated or inaccurate counts or predictions, or confusion during data retrieval |

### **Outdated data**

| Description                                                                            | Possible causes                                                               | Potential harm to businesses                        |
| ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ----------------------------------------------------- |
| Any data that is old which should be replaced with newer and more accurate information | People changing roles or companies, or software and systems becoming obsolete | Inaccurate insights, decision-making, and analytics |

### **Incomplete data**

| Description                               | Possible causes                                  | Potential harm to businesses                                                             |
| ------------------------------------------- | -------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| Any data that is missing important fields | Improper data collection or incorrect data entry | Decreased productivity, inaccurate insights, or inability to complete essential services |

### **Incorrect/inaccurate data**

| Description                              | Possible causes                                                        | Potential harm to businesses                                                              |
| ------------------------------------------ | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------- |
| Any data that is complete but inaccurate | Human error inserted during data input, fake information, or mock data | Inaccurate insights or decision-making based on bad information resulting in revenue loss |

### **Inconsistent data**

| Description                                                      | Possible causes                                                 | Potential harm to businesses                                                                 |
| ------------------------------------------------------------------ | ----------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |
| Any data that uses different formats to represent the same thing | Data stored incorrectly or errors inserted during data transfer | Contradictory data points leading to confusion or inability to classify or segment customers |

### **Business impact of dirty data**

For further reading on the business impact of dirty data, enter the term “dirty data” into your preferred browser’s search bar to bring up numerous articles on the topic. Here are a few impacts cited for certain industries from a previous search:

* **Banking**: Inaccuracies cost companies between 15% and 25% of revenue ([source](https://sloanreview.mit.edu/article/seizing-opportunity-in-data-quality/ "This link takes you to an MIT Sloan Management Review article.")).
* **Digital commerce:** Up to 25% of B2B database contacts contain inaccuracies ([source](https://www.demandgen.com/dirty-data-what-is-it-costing-you/ "This link takes you to a DemandGen blog article. ")).
* **Marketing and sales**: 99% of companies are actively tackling data quality in some way ([source](https://www.dqglobal.com/blog/why-bad-data-is-wasting-your-marketing-efforts/ "DQ Global: Why Bad Data is a Waste of Your Marketing Efforts")).
* **Healthcare**: Duplicate records can be 10% and even up to 20% of a hospital’s electronic health records ([source](https://searchhealthit.techtarget.com/feature/Hospitals-battle-duplicate-medical-records-with-technology "This link takes you to a TechTarget article for health IT.")).

# Common data-cleaning pitfalls

In this reading, you will learn the importance of data cleaning and how to identify common mistakes. Some of the errors you might come across while cleaning your data could include:

![list of common errors in data cleaning](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/xKinzzFmTkKop88xZs5C2Q_5da922d4cbd349e7aeb60ace3e18393a_Screen-Shot-2021-01-18-at-8.03.48-PM.png?expiry=1750896000000&hmac=6xOWrpFFhy8Dbmh-ioVxDm5FtPTfqyF-KIvJL3Vgg_I)

## Common mistakes to avoid

* **Not checking for spelling errors**: Misspellings can be as simple as typing or input errors. Most of the time the wrong spelling or common grammatical errors can be detected, but it gets harder with things like names or addresses. For example, if you are working with a spreadsheet table of customer data, you might come across a customer named “John” whose name has been input incorrectly as “Jon” in some places. The spreadsheet’s spellcheck probably won’t flag this, so if you don’t double-check for spelling errors and catch this, your analysis will have mistakes in it.
* **Forgetting to document errors**: Documenting your errors can be a big time saver, as it helps you avoid those errors in the future by showing you how you resolved them. For example, you might find an error in a formula in your spreadsheet. You discover that some of the dates in one of your columns haven’t been formatted correctly. If you make a note of this fix, you can reference it the next time your formula is broken, and get a head start on troubleshooting. Documenting your errors also helps you keep track of changes in your work, so that you can backtrack if a fix didn’t work.
* **Not checking for misfielded values**: A misfielded value happens when the values are entered into the wrong field. These values might still be formatted correctly, which makes them harder to catch if you aren’t careful. For example, you might have a dataset with columns for cities and countries. These are the same type of data, so they are easy to mix up. But if you were trying to find all of the instances of Spain in the country column, and Spain had mistakenly been entered into the city column, you would miss key data points. Making sure your data has been entered correctly is key to accurate, complete analysis.
* **Overlooking missing values**: Missing values in your dataset can create errors and give you inaccurate conclusions. For example, if you were trying to get the total number of sales from the last three months, but a week of transactions were missing, your calculations would be inaccurate.  As a best practice, try to keep your data as clean as possible by maintaining completeness and consistency.
* **Only looking at a subset of the data**: It is important to think about all of the relevant data when you are cleaning. This helps make sure you understand the whole story the data is telling, and that you are paying attention to all possible errors. For example, if you are working with data about bird migration patterns from different sources, but you only clean one source, you might not realize that some of the data is being repeated. This will cause problems in your analysis later on. If you want to avoid common errors like duplicates, each field of your data requires equal attention.
* **Losing track of business objectives**: When you are cleaning data, you might make new and interesting discoveries about your dataset-- but you don’t want those discoveries to distract you from the task at hand. For example, if you were working with weather data to find the average number of rainy days in your city, you might notice some interesting patterns about snowfall, too. That is really interesting, but it isn’t related to the question you are trying to answer right now. Being curious is great! But try not to let it distract you from the task at hand.
* **Not fixing the source of the error:** Fixing the error itself is important. But if that error is actually part of a bigger problem, you need to find the source of the issue. Otherwise, you will have to keep fixing that same error over and over again. For example, imagine you have a team spreadsheet that tracks everyone’s progress. The table keeps breaking because different people are entering different values. You can keep fixing all of these problems one by one, or you can set up your table to streamline data entry so everyone is on the same page. Addressing the source of the errors in your data will save you a lot of time in the long run.
* **Not analyzing the system prior to data cleaning:** If we want to clean our data and avoid future errors, we need to understand the root cause of your dirty data. Imagine you are an auto mechanic. You would find the cause of the problem before you started fixing the car, right? The same goes for data. First, you figure out where the errors come from. Maybe it is from a data entry error, not setting up a spell check, lack of formats, or from duplicates. Then, once you understand where bad data comes from, you can control it and keep your data clean.
* **Not backing up your data prior to data cleaning**: It is always good to be proactive and create your data backup before you start your data clean-up. If your program crashes, or if your changes cause a problem in your dataset, you can always go back to the saved version and restore it. The simple procedure of backing up your data can save you hours of work-- and most importantly, a headache.
* **Not accounting for data cleaning in your deadlines/process**: All good things take time, and that includes data cleaning. It is important to keep that in mind when going through your process and looking at your deadlines. When you set aside time for data cleaning, it helps you get a more accurate estimate for ETAs for stakeholders, and can help you know when to request an adjusted ETA.

# Workflow automation

In this reading, you will learn about workflow automation and how it can help you work faster and more efficiently. Basically, workflow automation is the process of automating parts of your work. That could mean creating an event trigger that sends a notification when a system is updated. Or it could mean automating parts of the data cleaning process. As you can probably imagine, automating different parts of your work can save you tons of time, increase productivity, and give you more bandwidth to focus on other important aspects of the job.

![An image of an alarm clock and an image of a person sitting at a desk with a computer](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/rgrPvRbvSxCKz70W74sQQw_3180a771074745fea045df7ce7340d86_Screen-Shot-2021-01-25-at-1.37.53-PM.png?expiry=1750896000000&hmac=fHySuNvCYag9591cUmlgNswkNawY6kdzhJlIFdWusBE)

## What can be automated?

Automation sounds amazing, doesn’t it? But as convenient as it is, there are still some parts of the job that can’t be automated. Let's take a look at some things we can automate and some things that we can’t.

| **Task**                                      | **Can it be automated?** | **Why?**                                                                                                                                                                                                                                                                                                          |
| ----------------------------------------------- | -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Communicating with your team and stakeholders | No                       | Communication is key to understanding the needs of your team and stakeholders as you complete the tasks you are working on. There is no replacement for person-to-person communications.                                                                                                                          |
| Presenting your findings                      | No                       | Presenting your data is a big part of your job as a data analyst. Making data accessible and understandable to stakeholders and creating data visualizations can’t be automated for the same reasons that communications can’t be automated.                                                                    |
| Preparing and cleaning data                   | Partially                | Some tasks in data preparation and cleaning can be automated by setting up specific processes, like using a programming script to automatically detect missing values.                                                                                                                                            |
| Data exploration                              | Partially                | Sometimes the best way to understand data is to see it. Luckily, there are plenty of tools available that can help automate the process of visualizing data. These tools can speed up the process of visualizing and understanding the data, but the exploration itself still needs to be done by a data analyst. |
| Modeling the data                             | Yes                      | Data modeling is a difficult process that involves lots of different factors; luckily there are tools that can completely automate the different stages.                                                                                                                                                          |

## More about automating data cleaning

One of the most important ways you can streamline your data cleaning is to clean data where it lives. This will benefit your whole team, and it also means you don’t have to repeat the process over and over. For example, you could create a programming script that counted the number of words in each spreadsheet file stored in a specific folder. Using tools that can be used where your data is stored means that you don’t have to repeat your cleaning steps, saving you and your team time and energy.

## More resources

There are a lot of tools out there that can help automate your processes, and those tools are improving all the time. Here are a few articles or blogs you can check out if you want to learn more about workflow automation and the different tools out there for you to use:

* Towards Data Science’s [**Automating Scientific Data Analysis**](https://towardsdatascience.com/automating-scientific-data-analysis-part-1-c9979cd0817e "This link takes you to a Toward Data Science article about automating data analysis with Python.")
* MIT News’ [**Automating Big-Data Analysis**](https://news.mit.edu/2016/automating-big-data-analysis-1021 "This link takes you to an MIT News article about automating data analysis.")
* TechnologyAdvice’s [**10 of the Best Options for Workflow Automation Software **](https://technologyadvice.com/blog/information-technology/top-10-workflow-automation-software/ "This link takes you to TechnologyAdvice's blog about the best workflow automation software.")
